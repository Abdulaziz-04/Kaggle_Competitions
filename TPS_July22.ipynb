{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/abdulaziz04/powertransformer-bgm?scriptVersionId=101166681\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.mixture import BayesianGaussianMixture\nfrom sklearn.preprocessing import RobustScaler,PowerTransformer\nfrom sklearn.decomposition import PCA\n\n\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-06T18:29:33.380343Z","iopub.execute_input":"2022-07-06T18:29:33.380759Z","iopub.status.idle":"2022-07-06T18:29:33.391971Z","shell.execute_reply.started":"2022-07-06T18:29:33.380726Z","shell.execute_reply":"2022-07-06T18:29:33.39078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the Dataset","metadata":{}},{"cell_type":"code","source":"sample=pd.read_csv('/kaggle/input/tabular-playground-series-jul-2022/sample_submission.csv')\nX=pd.read_csv('/kaggle/input/tabular-playground-series-jul-2022/data.csv')","metadata":{"execution":{"iopub.status.busy":"2022-07-06T18:29:33.413339Z","iopub.execute_input":"2022-07-06T18:29:33.413849Z","iopub.status.idle":"2022-07-06T18:29:34.244083Z","shell.execute_reply.started":"2022-07-06T18:29:33.413818Z","shell.execute_reply":"2022-07-06T18:29:34.242835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Storing the ids and dropping it from the feature Set","metadata":{}},{"cell_type":"code","source":"#Storing the ids for the final submission\nids=X['id']\n#Dropping it from the feature dataset\nX.drop('id',axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-06T18:29:34.245991Z","iopub.execute_input":"2022-07-06T18:29:34.246343Z","iopub.status.idle":"2022-07-06T18:29:34.259543Z","shell.execute_reply.started":"2022-07-06T18:29:34.246311Z","shell.execute_reply":"2022-07-06T18:29:34.258661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking for missing values\nX.info()","metadata":{"execution":{"iopub.status.busy":"2022-07-06T18:29:34.260887Z","iopub.execute_input":"2022-07-06T18:29:34.261409Z","iopub.status.idle":"2022-07-06T18:29:34.295979Z","shell.execute_reply.started":"2022-07-06T18:29:34.261376Z","shell.execute_reply":"2022-07-06T18:29:34.294761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Inertia : It measures how well a dataset was clustered by K-Means. It is calculated by measuring the distance between each data point and its centroid, squaring this distance, and summing these squares across one cluster. A good model is one with low inertia AND a low number of clusters ( K ).","metadata":{}},{"cell_type":"code","source":"#Storing the values of inertia\ninertias = []\n#Setting an estimated range for k values\nK = range(1, 15)\n  \nfor k in K:\n    # Building and fitting the model\n    kmeanModel = KMeans(n_clusters=k).fit(X)\n    kmeanModel.fit(X)\n    inertias.append(kmeanModel.inertia_)","metadata":{"execution":{"iopub.status.busy":"2022-07-06T18:29:34.29954Z","iopub.execute_input":"2022-07-06T18:29:34.300058Z","iopub.status.idle":"2022-07-06T18:32:10.289909Z","shell.execute_reply.started":"2022-07-06T18:29:34.300006Z","shell.execute_reply":"2022-07-06T18:32:10.288922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Attempt to find the optimal point of the cluster distribution using the well-known Elbow Method","metadata":{}},{"cell_type":"code","source":"plt.plot(K, inertias, 'bx-')\nplt.xlabel('Values of K')\nplt.ylabel('Inertia')\nplt.title('The Elbow Method using Inertia')\n#The graph appears to be a straight line after k=7\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-06T18:32:10.29152Z","iopub.execute_input":"2022-07-06T18:32:10.292494Z","iopub.status.idle":"2022-07-06T18:32:10.441377Z","shell.execute_reply.started":"2022-07-06T18:32:10.292444Z","shell.execute_reply":"2022-07-06T18:32:10.440189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's check the correlation heatmap using seaborn\n","metadata":{}},{"cell_type":"markdown","source":"## Normalizing data using RobustScaler and PowerTransformer\n","metadata":{}},{"cell_type":"code","source":"#Normalizing the dataset and also scale them\nrs=RobustScaler()\nX_rob=rs.fit_transform(X)","metadata":{"execution":{"iopub.status.busy":"2022-07-06T18:32:10.44316Z","iopub.execute_input":"2022-07-06T18:32:10.443831Z","iopub.status.idle":"2022-07-06T18:32:10.597563Z","shell.execute_reply.started":"2022-07-06T18:32:10.443788Z","shell.execute_reply":"2022-07-06T18:32:10.596376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pt=PowerTransformer()\nX_pt=pt.fit_transform(X_rob)","metadata":{"execution":{"iopub.status.busy":"2022-07-06T18:32:10.598868Z","iopub.execute_input":"2022-07-06T18:32:10.59929Z","iopub.status.idle":"2022-07-06T18:32:14.598412Z","shell.execute_reply.started":"2022-07-06T18:32:10.599256Z","shell.execute_reply":"2022-07-06T18:32:14.597346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#KMeans didn't give a better results as tested in previous versions, GMM works well\nbgm = BayesianGaussianMixture(n_components=7, covariance_type='full', random_state=1)\ny_pred = bgm.fit_predict(X_pt)","metadata":{"execution":{"iopub.status.busy":"2022-07-06T18:32:14.599865Z","iopub.execute_input":"2022-07-06T18:32:14.600228Z","iopub.status.idle":"2022-07-06T18:33:35.479021Z","shell.execute_reply.started":"2022-07-06T18:32:14.600196Z","shell.execute_reply":"2022-07-06T18:33:35.477507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building the Test Set","metadata":{}},{"cell_type":"code","source":"#building the test set\nresults=pd.DataFrame({'Id':ids,'Predicted':y_pred})\nresults.to_csv('submission.csv',index=False)\nresults","metadata":{"execution":{"iopub.status.busy":"2022-07-06T18:33:35.485889Z","iopub.execute_input":"2022-07-06T18:33:35.487792Z","iopub.status.idle":"2022-07-06T18:33:35.664881Z","shell.execute_reply.started":"2022-07-06T18:33:35.487718Z","shell.execute_reply":"2022-07-06T18:33:35.663844Z"},"trusted":true},"execution_count":null,"outputs":[]}]}